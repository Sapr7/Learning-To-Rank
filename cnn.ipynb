{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from datasets import *\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Data_for_transformer_cnn('datas_full_10k_1.pkl', which = 0, for_cnn=True)\n",
    "vali_data = Data_for_transformer_cnn('datas_full_10k_1.pkl', which = -1,for_cnn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "vali_loader = DataLoader(vali_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "         nn.Conv2d(1, 8, 8),\n",
    "         nn.ReLU(),\n",
    "         nn.Conv2d(8, 32, 8),\n",
    "         nn.ReLU(),\n",
    "         nn.Conv2d(32,64,8),\n",
    "         nn.ReLU(),\n",
    "         nn.Conv2d(64,64,8),\n",
    "         nn.ReLU(),\n",
    "         nn.Conv2d(64,32,8),\n",
    "         nn.ReLU(),\n",
    "         nn.Conv2d(32,16,8),\n",
    "         nn.ReLU(),\n",
    "         nn.Conv2d(16,8, 8),\n",
    "         nn.ReLU(),\n",
    "         nn.Flatten(),\n",
    "         nn.Linear(144072,16000),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(16000,1024),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(1024,512),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(512,256)         \n",
    "        )\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(p,y):\n",
    "    return -torch.mean(y * torch.log_softmax(p, dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aletovv/anaconda3/envs/v-env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = CNN_model().to(device)\n",
    "\n",
    "kl_loss = nn.KLDivLoss()\n",
    "\n",
    "opt_adam = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "m_loss = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_adam, mode='min', patience=3, factor=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 2322791520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_model(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(8, 8), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(8, 32, kernel_size=(8, 8), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 64, kernel_size=(8, 8), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(64, 64, kernel_size=(8, 8), stride=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(64, 32, kernel_size=(8, 8), stride=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(32, 16, kernel_size=(8, 8), stride=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(16, 8, kernel_size=(8, 8), stride=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): Flatten(start_dim=1, end_dim=-1)\n",
       "    (15): Linear(in_features=144072, out_features=16000, bias=True)\n",
       "    (16): ReLU()\n",
       "    (17): Linear(in_features=16000, out_features=1024, bias=True)\n",
       "    (18): ReLU()\n",
       "    (19): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (20): ReLU()\n",
       "    (21): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {model_total_params}')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vali(vali_loader, model, k = [5,10,None]):\n",
    "    model.eval()\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    acc = np.zeros(3)\n",
    "    length = len(vali_loader)\n",
    "    \n",
    "    for batch in vali_loader:\n",
    "        X = batch[0].to(device).unsqueeze(1)\n",
    "        y = batch[1].cpu().view(-1).numpy()\n",
    "            \n",
    "        preds = model(X.float()).view(-1).detach().cpu().numpy()\n",
    "        for j,i in enumerate(k):\n",
    "            try:\n",
    "                acc[j] += ndcg_score([list(y)], [list(preds)], k = i)\n",
    "            except:\n",
    "                length -= 1\n",
    "\n",
    "            \n",
    "    return acc / length\n",
    "\n",
    "def train(train_dataloader, test_dataloader, model, loss_fn, optimizer, sheluder, epochs, is_test = True, is_sheluder = False ):\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    pred_probs = nn.Softmax(dim = -1)\n",
    "    y_probs = nn.Softmax(dim = -1)\n",
    "    \n",
    "    train_loss = []\n",
    "    metric = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(epochs):\n",
    "        loss_epoch = 0\n",
    "        \n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            X = batch[0].float().to(device).unsqueeze(1)\n",
    "            y = batch[1].squeeze().float().to(device)\n",
    "            \n",
    "            preds = model(X).squeeze()\n",
    "            \n",
    "            probas_p = preds\n",
    "            probas_y = y\n",
    "            \n",
    "            optimizer.zero_grad()            \n",
    "            loss_ = loss_fn(probas_p, probas_y)\n",
    "            loss_.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_epoch += loss_\n",
    "          \n",
    "        \n",
    "        train_loss.append(loss_epoch.clone().detach().cpu().numpy()/len(train_dataloader))\n",
    "        \n",
    "        if is_test:\n",
    "            model.eval()\n",
    "            metric.append(vali(vali_loader=test_dataloader, model = model))\n",
    "            print(f'loss for epoch {i+1} is {train_loss[i]:.4f} ||for epoch{i+1} ndcg@5_score is {metric[i][0]:.4f} || ndcg@10_score is {metric[i][1]:.4f} || ndcg_score is {metric[i][2]:.4f}  ')\n",
    "        else:    \n",
    "            print(f'loss for epoch {i+1} is {train_loss[i]:.4f}')\n",
    "            \n",
    "        if is_sheluder:   \n",
    "            sheluder.step(loss_epoch)\n",
    "            \n",
    "    return np.array(train_loss), np.array(metric)\n",
    "    \n",
    "        \n",
    "def plot_loss_and_metric(train_loss, name, is_metric = True, metric = []):\n",
    "    size = len(train_loss)\n",
    "    \n",
    "    if is_metric:\n",
    "        fig, ax = plt.subplots(1,4, figsize = (20,6), constrained_layout=True)\n",
    "        \n",
    "        \n",
    "        ax[0].set_yscale('log')\n",
    "        ax[0].plot(np.arange(size), train_loss)\n",
    "        ax[0].set_xlabel('epochs')\n",
    "        ax[0].set_ylabel('loss')\n",
    "        ax[0].set_title('train loss')\n",
    "        ax[0].grid(True)\n",
    "        \n",
    "        for i,j  in enumerate([5,10,'']):\n",
    "        \n",
    "            ax[i+1].plot(np.arange(size), metric[:,i])\n",
    "            ax[i+1].set_xlabel('epochs')\n",
    "            ax[i+1].set_ylabel(f'ndcg@{j}_score')\n",
    "            ax[i+1].set_title('eval metric')\n",
    "            ax[i+1].grid(True)\n",
    "\n",
    "        \n",
    "        fig.savefig(name)\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize = (6,6))\n",
    "        fig.set_figheight(8)\n",
    "        fig.set_figwidth(12)\n",
    "        \n",
    "        ax.set_yscale('log')\n",
    "        ax.plot(np.arange(size), train_loss)\n",
    "        ax.set_xlabel('epochs')\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.set_title('train loss')\n",
    "        ax.grid(True)\n",
    "                \n",
    "        fig.savefig(name)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 846.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 320.69 MiB is free. Including non-PyTorch memory, this process has 10.43 GiB memory in use. Of the allocated memory 10.23 GiB is allocated by PyTorch, and 12.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses \u001b[38;5;241m=\u001b[39m train(train_dataloader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m      2\u001b[0m                test_dataloader\u001b[38;5;241m=\u001b[39m vali_loader,\n\u001b[1;32m      3\u001b[0m       model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m      4\u001b[0m       loss_fn\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m      5\u001b[0m       optimizer\u001b[38;5;241m=\u001b[39mopt_adam,sheluder\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[1;32m      6\u001b[0m       epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, test_dataloader, model, loss_fn, optimizer, sheluder, epochs, is_test, is_sheluder)\u001b[0m\n\u001b[1;32m     38\u001b[0m X \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 41\u001b[0m preds \u001b[38;5;241m=\u001b[39m model(X)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     43\u001b[0m probas_p \u001b[38;5;241m=\u001b[39m preds\n\u001b[1;32m     44\u001b[0m probas_y \u001b[38;5;241m=\u001b[39m y\n",
      "File \u001b[0;32m~/anaconda3/envs/v-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/v-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mCNN_model.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,X):\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(X)\n",
      "File \u001b[0;32m~/anaconda3/envs/v-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/v-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/v-env/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/v-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/v-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/v-env/lib/python3.12/site-packages/torch/nn/modules/activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[0;32m~/anaconda3/envs/v-env/lib/python3.12/site-packages/torch/nn/functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 846.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 320.69 MiB is free. Including non-PyTorch memory, this process has 10.43 GiB memory in use. Of the allocated memory 10.23 GiB is allocated by PyTorch, and 12.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "losses = train(train_dataloader=train_loader,\n",
    "               test_dataloader= vali_loader,\n",
    "      model = model,\n",
    "      loss_fn=loss,\n",
    "      optimizer=opt_adam,sheluder=scheduler,\n",
    "      epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 256, 136])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    \n",
    "    x = i[0].to(device).float().unsqueeze(1)\n",
    "    print(x.shape)\n",
    "    model(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
